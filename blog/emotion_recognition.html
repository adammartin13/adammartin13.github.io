<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <title>Emotion Recognition</title>
        <link rel="icon" type="image/x-icon" href="./assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Varela+Round" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Nunito:200,200i,300,300i,400,400i,600,600i,700,700i,800,800i,900,900i" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="../Home.html">Home</a>
                <a class="navbar-brand" style="padding-left:1.5rem;" href="../Software.html">Software</a>
                <a class="navbar-brand" style="padding-left:1.5rem;" href="../Writing.html">Writing</a>
                <a class="navbar-brand" style="padding-left:1.5rem;" href="../Photography.html">Photography</a>
            </div>
        </nav>
        <!-- Page header -->
        <header class="masthead emotion_recognition">
            <div class="container px-4 px-lg-5 d-flex h-100">
                <div class="d-flex my-auto">
                    <div>
                        <div>
                            <h1 class="mx-auto my-0 text-uppercase blog-title">Emotion Recognition</h1>
                            <h3 class="text-white-50 mx-auto mt-2 mb-5 blog-sub">An attempt to mathematically deduce emotions using facial recognition data.</h3>
                            <h2 class="text-white-50 mt-2 mb-5 blog-meta">Summer 2021</h2>
                        </div>
                        <a class="btn btn-primary" href="#start">Read</a>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <article class="mb-4" id="start">
            <div class="container px-4 px-lg-5 my-4">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <p>
                            It's late Spring in a college apartment dimly lit by a TV. I’m exchanging the evening homework study for a 
                            crack at inspiration, watching Ex Machina. I was a film snob and a sophomore coder coming off the back of a 
                            failed project. I needed ideas, but tonight was just another movie. Enjoying cinema that wasn’t another cape 
                            flick.
                        </p>
                        <p>
                            Our protagonist Caleb enters the glass chamber for his third interview with Ava. His mannerisms and 
                            body language indicate a strange disbelief at the android on the other side of the cell and his feelings towards 
                            her. Ava makes an attempt to reach him. <i>Are you attracted to me?</i> Caleb doesn’t know how to respond. <i>You give me 
                            indications that you care. Microexpressions. The way your eyes fix on my eyes and lips. The way you hold my gaze.</i>
                        </p>
                        <p>
                            In the LCD blue light I opened myself to late night thoughts on AI. Would it exhibit full automata? Does an AGI have rights?
                            If it can not feel in the same way that we do, through biochemical hormonal processes, then could it at least attempt to 
                            understand emotions? If it could, then would it change its output in the same way that our conversations evolve with the 
                            mood and atmosphere surrounding the discussions framework? I believed that AGI was capable of 
                            representing the full spectrum of human emotion, thought, and creativity.
                        </p>
                        <p>
                            I found Caleb’s attachment to Ava suspect. Why would someone share these feelings with an android? Even if Ava exhibited full 
                            autonomy so as to say that she is merely an artificial human, why would he fall for someone that he’s only met a handful of 
                            times and knows so little of? Given that it is later revealed that Nathan, Ava’s creator, designed Ava around Caleb’s porn 
                            history and chose Caleb for his moral compass and perhaps his relationship situation, is it not out of the question that Nathan 
                            perhaps exploited Caleb’s emotions for his experiment? That Ava was in fact, able to understand Caleb’s emotions at such a high 
                            level that it could be weaponized to her advantage.
                        </p>
                        <p>
                            It stuck to me. I enjoyed the dystopian thriller, the emotional manipulation of our characters, the artistic expression in color 
                            identity and theological representations. But that word, <i>microexpressions</i>, lingered like morbid curiosity. She was able to 
                            manipulate him because she had an understanding of his emotions, but how did she understand his emotions? Was it purely from 
                            microexpressions or are there other ways in which we express how we feel? In the present sense, what tools do LLM’s use today to 
                            utilize our emotions or do such tools even exist? In the quiet dark of that late night college apartment, I then found my new 
                            project.
                        </p>
                        <h2 class="section-heading">Skin-Deep</h2>
                        <p>
                            I went straight to the drawing board. Like any project I’ve pursued I was nose diving into unfamiliar territory. I knew 
                            large language models were essentially vectorizing text as part of some training measure, but whether or not emotion could be 
                            represented as an additional input vector was unknown to me. What I could infer however is that before I could ever enhance a 
                            large language model through this capacity, I would first need the tools necessary to understand human emotion.
                        </p>
                        <p>
                            This was my first software project that I felt transcended the realm of simply coding a solution to a problem that dipped into 
                            learning new subject matters I’ve never studied before. I was combining coding with psychology and anatomy. It presented this vastly 
                            entertaining quest where I allowed myself to learn more about my skills while also discovering people in a manner I’ve never 
                            thought about previously.
                        </p>
                        <p>
                            I read several papers, some of which indicated that the spectrum of emotions could be determined via 
                            <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7037130/">brain waves, skin conductivity, and heart rate.</a>
                            In particular I was interested in 
                            <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2342913/#!po=15.7407">facial expression associations and their relative muscle activations.</a>
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/face_muscle.png">
                        <p class="figure-caption text-muted">
                            Central Control of Muscles of Facial Expression, Fig. 1. J A Stephens, A A Root
                        </p>
                        <p>
                            If we were to apply mainstream technology towards our emotion recognition solution, then facial recognition seemed like the 
                            simplest approach given smart phone accessibility. With a prioritization on facial recognition for analyzing emotion, the next 
                            step would be to write a program for getting us facial data.
                        </p>
                        <p>
                            To this end I ended up using Adam Geitgey’s 
                            <a href="https://pypi.org/project/face-recognition/">face-recognition</a> 
                            library. This library utilized dlib’s recognition to assist us with 
                            identifying facial landmarks. Once I can identify the key points I’m looking for in conjunction with the research papers I’ve found, I 
                            can move towards mapping out facial muscles in association with these landmark locations.
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/face_points.png">
                        <p class="figure-caption text-muted">
                            DLIB Facial Landmarks
                        </p>
                        <p>
                            I took a shot at this on a hot weekend back home. There’s a python function that I wrote using this 
                            library; I enabled the webcam on my laptop, set some color preferences, and the function ran faster than 
                            expected. My project directory contained two new image files.
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/output.png" style="margin-bottom: 1rem;">
                        <p>
                            It's not common unless you’re a front end developer to receive immediate visual feedback on your code. The 
                            code wasn’t doing anything new, I just modified the output, but it felt so invigorating to see my face 
                            get captured and these datapoints populate. I had performed facial recognition with dlib 
                            and I saw that it was possible to isolate these facial data points. I felt that this project might 
                            be something that I could accomplish.
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/code1.png" style="margin-bottom: 1rem;">
                        <h2 class="section-heading">It Grows from Within</h2>
                        <p>
                            I got back to the books. My program will need to identify the 
                            <a href="https://www.paulekman.com/universal-emotions/">seven universal expressions</a>
                            (anger, contempt, disgust, fear, happiness, sadness, and surprise) as well as an eighth “neutral” 
                            expression. The easiest of these being happiness which we can associate with the activation of the 
                            zygomaticus major (ZMM). 
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/ZMM.png">
                        <p class="figure-caption text-muted">
                            Investigating the Contraction Pattern of the Zygomaticus Major, Fig. 4. Daniel J. Rams.
                        </p>
                        <p>
                            To track this muscle we simply need to record a face and then extract the proper points associated with the 
                            muscle. In our case no advanced geometry would be required, since there is already a tracked landmark for 
                            the edges of the mouth (48 and 54) which we can then draw a line from to their respective upper jaw 
                            landmark points (0 and 16). This gives us a rough estimate of where the ZMM is. Given a video input where 
                            we can measure the change in the ZMM relative to time, we can determine if at a given frame the ZMM is 
                            activated and if so, infer that the user is smiling and thus happy.
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/code2.png" style="margin-bottom: 1rem;">
                        <p>
                            The image below is the result of applying our code wherein we sketch the facial landmarks with cyan and the 
                            ZMM with violet over a medical diagram consisting of a portrait and a depiction of the ZMM muscle over 
                            the person's face. The program is able to draw the line through the ZMM. Contraction of the muscle would 
                            result in the length of the line decreasing, thus allowing us to determine if the user is smiling. This 
                            supports our earlier hypothesis where it may in fact be possible for LLM’s to understand human emotion 
                            via the mathematical deduction of facial structures in response to particular biochemical signatures. If 
                            the user is happy they will smile and we can calculate that they are smiling. An AI may not be able to 
                            feel happiness in the same way that we do, but it’s possible for it to understand that we are happy.
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/ZMM_Diagram.png" style="margin-bottom: 1rem;">
                        <p>
                            After drawing the line in our output we determine the size of the ZMM using the distance formula between 
                            the two points, allowing us to track each ZMM activation separately. This however introduces a new 
                            problem: we’re only determining the size of a muscle in pixel space. Effectively any paper that we’ll 
                            read on the subject of muscle contraction will provide us a metric response in regards to the size of the 
                            contracted muscle. This leads us into an age-old problem of real space in computer vision. So now we have 
                            the new problem of determining the metric size of facial components given an image.
                        </p>
                        <p>
                            At the time of developing this project, smartphone software for mapping a face in 3D space from a series of
                            images did not exist. Thus to obtain the size of an object on a 2D plane would require some point of 
                            reference, say a quarter. Having such a point of reference would allow me to create a pixel to metric 
                            ratio. The question I raised then was would it be possible to forgo the static reference object for a 
                            standard image of the face without the use of additional technologies or metadata?
                        </p>
                        <h2 class="section-heading">Window to the Soul</h2>
                        <p>
                            My assumption: the human eye is one of the least growing organs on the human body. Its size is also roughly
                            the same regardless of other genetic factors across all humans. Can we measure the pixel size of the 
                            user's eye, then cross-reference other anatomy papers to determine a pixel to metric ratio that can be 
                            used to determine the size of other facial features?
                        </p>
                        <p>
                            The “size of the eye”, or rather the size of the exposed eye can be determined via the palpebral fissure 
                            length (PFL). This can be measured via the distance from the edges of the lacrimal caruncle to the 
                            lateral commissure. Since facial recognition models often base their assumptions around changes in 
                            shade/color, our model does a good job at isolating these points at 36/39 for the left eye and 42/45 for 
                            the right.
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/PFL.png">
                        <p class="figure-caption text-muted">
                            Appl. of Deep Learning for Investigation of Neurological Diseases, Anatomy of the Eye. Mohammed Hamoud.
                        </p>
                        <p>
                            A quick Google search however on how long exactly the PFL is yields signs of ethnic bias. For instance, the 
                            Wikipedia entry dictates that the PFL is “approximately 30mm horizontally” which appears to coincide with 
                            <a href="https://depts.washington.edu/fasdpn/pdfs/FAR011002_e231-e241_Astley%5B1%5D.pdf">this study</a> 
                            from the University of Washington of ~500 participants who were all Caucasian. I then proceeded to read 
                            several papers that conducted this study across their respective ethnic and gender groups. My results 
                            were the following:
                        </p>
                        <table class="table">
                            <tbody>
                              <tr>
                                <td>Chinese Male</td>
                                <td>23.9mm</td>
                                <td>Chinese Female</td>
                                <td>23.2mm</td>
                              </tr>
                              <tr>
                                <td>Indian Male</td>
                                <td>29.1mm</td>
                                <td>Indian Female</td>
                                <td>27.4mm</td>
                              </tr>
                              <tr>
                                <td>Black Male</td>
                                <td>32.3mm</td>
                                <td>Black Female</td>
                                <td>31.5mm</td>
                              </tr>
                              <tr>
                                <td>White Male</td>
                                <td>29.5mm</td>
                                <td>White Female</td>
                                <td>29.4mm</td>
                              </tr>
                            </tbody>
                          </table>
                        <p>
                            Utilizing this information would allow us to more accurately portray the sizing of particular muscles when 
                            using PFL as a point of reference. We can then find the PFL in a similar manner to the ZMM:
                        </p>
                        <img class="img-fluid" src="/blog/emotion_recognition/code3.png" style="margin-bottom: 1rem;">
                        <p>
                            I could then begin tracking ZMM contraction and by fall of 2021 compute our first emotion: happiness.
                        </p>
                        <h2 class="section-heading">Onward</h2>
                        <p>
                            In the winter I conducted a study where I gathered data on 100 universities and their respective computer 
                            science programs, writing an algorithm to tell me who I should transfer to following my sophomore year at 
                            Francis Marion University. In the summer of 2022 I packed my bags and moved across the country to Iowa 
                            State University. Over the year I applied to over 50 research programs and landed an opportunity at Old 
                            Dominion University to build a large language model that could be used to deter Russian disinformation.
                        </p>
                        <p>
                            I haven’t gotten back to this project since beginning my work with my research partner Iryna on this 
                            Russian disinformation model. I was given the opportunity to build something that has a chance of 
                            actually helping people, rather than designing a tool for measuring them. Hearing her stories I could see 
                            that this project was more than just a PhD dissertation. Being a Ukrainian migrant herself this was about 
                            her, her family, and her country. Our likelihood of success was irrelevant to me. Through her stories I 
                            could see a catching determination to try and help people and I couldn’t help but feel the same.
                        </p>
                        <p>
                            It’s spring of 2025 as of writing this and I want to finish this project once I’m done with the 
                            disinformation model. I feel that while the objective is the same the purpose is entirely different. I 
                            want to design this program not for the betterment of an AGI, but to find applications where measuring 
                            emotion can help people or perhaps bring us closer together. Technologies have changed and developments 
                            have been made in mapping 3D space from image input. The possibilities have broadened greatly since my 
                            time away from this project and I’m all the more excited to return. When I do I think I’ll finally help a 
                            computer understand how we feel.
                        </p>
                    </div>
                </div>
            </div>
        </article>
        <!-- Contact-->
        <section class="contact-section bg-black" id="contact">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5">
                    <div class="col-md-4 mb-3 mb-md-0">
                        <div class="card py-4 h-100">
                            <div class="card-body text-center">
                                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github text-primary mb-2" viewBox="0 0 16 16">
                                    <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8"/>
                                </svg>
                                <h4 class="text-uppercase m-0">GitHub</h4>
                                <hr class="my-4 mx-auto" />
                                <div class="small card-text"><a href="https://github.com/adammartin13">github.com/adammartin13</a></div>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-4 mb-3 mb-md-0">
                        <div class="card py-4 h-100">
                            <div class="card-body text-center">
                                <i class="fas fa-envelope text-primary mb-2"></i>
                                <h4 class="text-uppercase m-0">Email</h4>
                                <hr class="my-4 mx-auto" />
                                <div class="small card-text">cadamm@iastate.edu</div>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-4 mb-3 mb-md-0">
                        <div class="card py-4 h-100">
                            <div class="card-body text-center">
                                <i class="fas fa-mobile-alt text-primary mb-2"></i>
                                <h4 class="text-uppercase m-0">Download Resume</h4>
                                <hr class="my-4 mx-auto" />
                                <a href="../Resume.pdf" target="_blank" class="small card-text">Download</a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="../js/scripts.js"></script>
    </body>
</html>
